{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:04:55.150651Z",
     "start_time": "2024-02-02T15:04:54.644548Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:05:04.005958Z",
     "start_time": "2024-02-02T15:05:04.001943Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use current path as the root path\n",
    "__file__ = Path.cwd()\n",
    "ROOT_PATH = Path.cwd()\n",
    "\n",
    "# other directories\n",
    "LOG_PATH = (ROOT_PATH / \"log\")\n",
    "DATA_PATH = (ROOT_PATH / 'data')\n",
    "SRC_PATH = (ROOT_PATH / 'src')\n",
    "CONFIG_PATH = (ROOT_PATH / 'configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:05:04.943983Z",
     "start_time": "2024-02-02T15:05:04.935029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load YAML validation schema\n",
    "config_path = CONFIG_PATH / 'main.yaml'\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    rules = yaml.safe_load(file)['table_schema']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:05:56.465713Z",
     "start_time": "2024-02-02T15:05:56.451730Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID1</th>\n",
       "      <th>Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Signup Date</th>\n",
       "      <th>Last Purchase</th>\n",
       "      <th>Total Spent</th>\n",
       "      <th>Referral Code</th>\n",
       "      <th>Membership Level1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID1  Name  Email  Signup Date  Last Purchase  Total Spent  Referral Code  \\\n",
       "0    1     1      1            1              1            1              1   \n",
       "1    1     1      1            1              1            1              1   \n",
       "\n",
       "   Membership Level1  \n",
       "0                  1  \n",
       "1                  1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_01 = pd.read_excel(\"data/test_01.xlsx\")\n",
    "test_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doorkeeper:\n",
    "    def __init__(self, ) -> None:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:06:07.960838Z",
     "start_time": "2024-02-02T15:06:07.954831Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to validate a DataFrame against the rules\n",
    "def validate_table(df, rules):\n",
    "    errors = []\n",
    "\n",
    "    # Check required columns\n",
    "    for col in rules['required_columns']:\n",
    "        if col['name'] not in df.columns:\n",
    "            errors.append(f\"Missing required column: {col['name']}\")\n",
    "        # else:\n",
    "        #     # Validate data type and emptiness\n",
    "        #     if not all(df[col['name']].apply(lambda x: isinstance(x, eval(col['data_type'])) if pd.notnull(x) else col['allows_empty'])):\n",
    "        #         errors.append(f\"Invalid data in column: {col['name']}\")\n",
    "\n",
    "    # Check for optional columns constraints if they exist\n",
    "    for col in rules.get('optional_columns', []):\n",
    "        if col['name'] in df.columns:\n",
    "            # Similar validation as for required columns can be performed here\n",
    "            pass\n",
    "\n",
    "    # Additional constraints\n",
    "    # for constraint in rules.get('constraints', []):\n",
    "    #     if 'regex' in constraint and constraint['column'] in df.columns:\n",
    "    #         if not all(df[constraint['column']].apply(lambda x: re.match(constraint['regex'], x) if pd.notnull(x) else True)):\n",
    "    #             errors.append(f\"Data in column {constraint['column']} fails regex validation\")\n",
    "    #\n",
    "    #     if 'unique' in constraint and constraint['column'] in df.columns:\n",
    "    #         if not df[constraint['column']].is_unique:\n",
    "    #             errors.append(f\"Data in column {constraint['column']} is not unique as required\")\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:06:08.464969Z",
     "start_time": "2024-02-02T15:06:08.463001Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = validate_table(df=test_01, rules=rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:06:08.780103Z",
     "start_time": "2024-02-02T15:06:08.775158Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Missing required column: ID']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ipykernel import get_connection_file\n",
    "\n",
    "def get_notebook_path():\n",
    "    connection_file = get_connection_file()\n",
    "    connection_file_dirname = os.path.dirname(connection_file)\n",
    "    notebooks = [nb for nb in os.listdir(connection_file_dirname) if nb.endswith(\".ipynb\")]\n",
    "    if notebooks:\n",
    "        return os.path.join(connection_file_dirname, notebooks[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(get_notebook_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the core idea is that each template can be used to validate multiple datasets\n",
    "datasets = {\n",
    "    \"data1\": {\n",
    "        \"template\": \"templates/data1.yaml\",\n",
    "        \"data\": [\"data/data1_01.csv\", \"data/data_02.xlsx\"]\n",
    "    },\n",
    "    \"data2\": {\n",
    "        \"template\": \"templates/data2.yaml\",\n",
    "        \"data\": [\"data/data2_01.csv\", \"data/data2_02.csv\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# each task can contain multiple templates and datasets\n",
    "class Task:\n",
    "    def __init__(self, task:str) -> None:\n",
    "        self.TASK = task\n",
    "        self.TIME_TAG = self.get_current_datetime()\n",
    "\n",
    "        # file paths\n",
    "        self.PATH = {\n",
    "            \"ROOT\": Path.cwd(),\n",
    "            \"DATA\": (ROOT_PATH / 'data'),\n",
    "            \"TEMPLATES\": (ROOT_PATH / 'templates'),\n",
    "            \"TASK_TEMPLATES\": (ROOT_PATH / 'templates' / 'tasks'),\n",
    "            \"DATASET_TEMPLATES\": (ROOT_PATH / 'templates' / 'datasets'),\n",
    "        }\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def get_current_datetime(self):\n",
    "        \"\"\"\n",
    "        get current date and time as string\n",
    "        \"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def create_paths(self):\n",
    "        \"\"\"\n",
    "        Crate the necessary directories\n",
    "        \"\"\"\n",
    "        for path in self.PATH.values():\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def load_task_template(self):\n",
    "        \"\"\"\n",
    "        Load the task template\n",
    "        \"\"\"\n",
    "        path = self.PATH[\"TASK_TEMPLATES\"] / f\"{self.TASK}.yaml\"\n",
    "        with open(path, 'r') as file:\n",
    "            return yaml.safe_load(file)\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Load the datasets from the data directory\n",
    "        \"\"\"\n",
    "        for dataset in self.datasets:\n",
    "            self.datasets[dataset] = pd.read_csv(self.PATH[\"DATA\"] / dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
